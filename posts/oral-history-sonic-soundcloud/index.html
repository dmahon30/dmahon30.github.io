<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>My Digital Mewsings Blog</title>
<meta name="keywords" content="" />
<meta name="description" content="Oral History Sonic Soundcloud: Experimentation DevLog 2: The Struggle to Sonify Transcript Data is Real. The Initial Plans (Spoiler: There are 2)   Plan number one: I wanted to take workforce data from the Africville Relocation Report and sonify that to interpret a conversation about labour data between genders.
  Plan number two: The first plan was simply to challenge myself to work with quantitative data to make an argument through sonal interpretation (rather than the usual essay format of historical argumentation).">
<meta name="author" content="">
<link rel="canonical" href="https://dmahon30.github.io/posts/oral-history-sonic-soundcloud/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://dmahon30.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dmahon30.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dmahon30.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dmahon30.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://dmahon30.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="Oral History Sonic Soundcloud: Experimentation DevLog 2: The Struggle to Sonify Transcript Data is Real. The Initial Plans (Spoiler: There are 2)   Plan number one: I wanted to take workforce data from the Africville Relocation Report and sonify that to interpret a conversation about labour data between genders.
  Plan number two: The first plan was simply to challenge myself to work with quantitative data to make an argument through sonal interpretation (rather than the usual essay format of historical argumentation)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dmahon30.github.io/posts/oral-history-sonic-soundcloud/" /><meta property="article:section" content="posts" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Oral History Sonic Soundcloud: Experimentation DevLog 2: The Struggle to Sonify Transcript Data is Real. The Initial Plans (Spoiler: There are 2)   Plan number one: I wanted to take workforce data from the Africville Relocation Report and sonify that to interpret a conversation about labour data between genders.
  Plan number two: The first plan was simply to challenge myself to work with quantitative data to make an argument through sonal interpretation (rather than the usual essay format of historical argumentation)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dmahon30.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "https://dmahon30.github.io/posts/oral-history-sonic-soundcloud/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "Oral History Sonic Soundcloud: Experimentation DevLog 2: The Struggle to Sonify Transcript Data is Real. The Initial Plans (Spoiler: There are 2)   Plan number one: I wanted to take workforce data from the Africville Relocation Report and sonify that to interpret a conversation about labour data between genders.\n  Plan number two: The first plan was simply to challenge myself to work with quantitative data to make an argument through sonal interpretation (rather than the usual essay format of historical argumentation).",
  "keywords": [
    
  ],
  "articleBody": "Oral History Sonic Soundcloud: Experimentation DevLog 2: The Struggle to Sonify Transcript Data is Real. The Initial Plans (Spoiler: There are 2)   Plan number one: I wanted to take workforce data from the Africville Relocation Report and sonify that to interpret a conversation about labour data between genders.\n  Plan number two: The first plan was simply to challenge myself to work with quantitative data to make an argument through sonal interpretation (rather than the usual essay format of historical argumentation).\n Plan number two involved A. Analyzing qualitative data (interview transcripts with former residents) using TF-IDF (via Voyant or R-language) B. Creating a visual and audible interpretation of the 25 most frequent/significant terms in the transcript data.    Challenges \u0026 Change in Plans   OK, so things did not go according to plan.\n  Re: PLAN 1:\n The MIDIscript detailed in Programming Historian on Sonification really relies on change over time. The issues with my data was that the information was drawn from the same year, plus the interpretation on workforce data was limited in scope and quantity. I didn’t feel comfortable manipulating the time variables, so I had to abandon that dataset. Instead, I’ll be experimenting with statscan census data on ethnicity in Canada from the late 19th-century to the mid-20th century. What stood out to me about this data (compiled in the early 1970s) was the conception of ethnicity measured across the years. What sort of conversation can we glean from how ethnicity was categorized, measured, and silenced - how can we sonify [this data] (https://docs.google.com/spreadsheets/d/1PEKXc-f0oEbr5uTKmYaYvoL4BSn8zBORAMbI0Yspqgs/edit?usp=sharing)? So I’ll try again with the python script using the above data and see what I can come up with!    Re: PLAN 2:\n DeepSpeech is not picking up the words effectively in it’s speech-to-text function (no matter how small of a segment I make the interviews). So, moving forward, I think it would be faster for me to transcribe each interview by hand. I’ll transcribe 2-3 one hour interviews in order to have a corpus of texts for the TF-IDF function to work + [This StackOverFlow thread] (https://stackoverflow.com/questions/22790974/how-to-calculate-tfidf-for-a-single-new-document-to-be-classified) notes that the text function would work fine for one text, but the inverse-document-frequency function wouldn’t apply as easily. Since I had to switch gears with DeepSpeech, I only have one transcript to work with at this time. So then I ran it through Voyant to get some text frequency data to create a prototype, but I like the customization option of using R so that will be the tool I use for the corpus.    Why use TF-IDF for Oral History Transcripts?  I know there are limitations in both the tool and using transcripts to identify textual patterns. Computational analysis tools can be a great way to help users get a feel for the content of an OH collection. It can present a powerful means of understanding document similarity by identifying key terms across multiple transcripts. Unfortunately I don’t have a prototype resulting from the IDF application, but I do have one showing TF patterns in one interview.  Plan 2: Prototype Process  After running the transcript through Voyant, I edited the stopwords to remove verbal pauses, and other non-descript terms. This generated a list of the 25 most frequent terms in the text, as well as a wordcloud representation of the ascending data. If you’d like to see my list of stopwords and the frequency list, click here  Here is the visualized data (the wordcloud) Now, the audio prototype   I decided I wanted to create an audiofile that reflected the frequency data, which I made using only the 5 most frequent terms.\n  I then pulled samples from my interview where the participants were saying these words to add another dimension to the data.\n  I also thought it would be quite powerful to loop these words, and increase the volume for each word depending on their TF. For example, the more often the word appeared in the text, the louder the word would be. The audio was then ordered from the quietest sample to the loudest.\n  This is a very basic prototype, but here is an example of the sonal interpretation:\n    ",
  "wordCount" : "682",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dmahon30.github.io/posts/oral-history-sonic-soundcloud/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Digital Mewsings Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dmahon30.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dmahon30.github.io/" accesskey="h" title="My Digital Mewsings Blog (Alt + H)">My Digital Mewsings Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><h1 id="oral-history-sonic-soundcloud-experimentation">Oral History Sonic Soundcloud: Experimentation<a hidden class="anchor" aria-hidden="true" href="#oral-history-sonic-soundcloud-experimentation">#</a></h1>
<h2 id="devlog-2-the-struggle-to-sonify-transcript-data-is-real">DevLog 2: The Struggle to Sonify Transcript Data is Real.<a hidden class="anchor" aria-hidden="true" href="#devlog-2-the-struggle-to-sonify-transcript-data-is-real">#</a></h2>
<h2 id="the-initial-plans-spoiler-there-are-2">The Initial Plans (Spoiler: There are 2)<a hidden class="anchor" aria-hidden="true" href="#the-initial-plans-spoiler-there-are-2">#</a></h2>
<ul>
<li>
<p>Plan number one: I wanted to take workforce data from the Africville Relocation Report and sonify that to interpret a conversation about labour data between genders.</p>
</li>
<li>
<p>Plan number two: The first plan was simply to challenge myself to work with quantitative data to make an argument through sonal interpretation (rather than the usual essay format of historical argumentation).</p>
<ul>
<li>Plan number two involved
A. Analyzing qualitative data (interview transcripts with former residents) using TF-IDF (via Voyant or R-language)
B. Creating a visual and audible interpretation of the 25 most frequent/significant terms in the transcript data.</li>
</ul>
</li>
</ul>
<h2 id="challenges--change-in-plans">Challenges &amp; Change in Plans<a hidden class="anchor" aria-hidden="true" href="#challenges--change-in-plans">#</a></h2>
<ul>
<li>
<p>OK, so things did not go according to plan.</p>
</li>
<li>
<p>Re: PLAN 1:</p>
<ul>
<li>The MIDIscript detailed in <a href="https://programminghistorian.org/en/lessons/sonification">Programming Historian on Sonification</a> really relies on change over time. The issues with my data was that the information was drawn from the same year, plus the interpretation on workforce data was limited in scope and quantity. I didn&rsquo;t feel comfortable manipulating the time variables, so I had to abandon that dataset. Instead, I&rsquo;ll be experimenting with statscan census data on ethnicity in Canada from the late 19th-century to the mid-20th century.</li>
<li>What stood out to me about this data (compiled in the early 1970s) was the conception of ethnicity measured across the years. What sort of conversation can we glean from how ethnicity was categorized, measured, and silenced - how can we sonify [this data] (<a href="https://docs.google.com/spreadsheets/d/1PEKXc-f0oEbr5uTKmYaYvoL4BSn8zBORAMbI0Yspqgs/edit?usp=sharing)?">https://docs.google.com/spreadsheets/d/1PEKXc-f0oEbr5uTKmYaYvoL4BSn8zBORAMbI0Yspqgs/edit?usp=sharing)?</a></li>
<li>So I&rsquo;ll try again with the python script using the above data and see what I can come up with!</li>
</ul>
</li>
<li>
<p>Re: PLAN 2:</p>
<ul>
<li>DeepSpeech is not picking up the words effectively in it&rsquo;s speech-to-text function (no matter how small of a segment I make the interviews). So, moving forward, I think it would be faster for me to transcribe each interview by hand. I&rsquo;ll transcribe 2-3 one hour interviews in order to have a corpus of texts for the TF-IDF function to work
+ [This StackOverFlow thread] (<a href="https://stackoverflow.com/questions/22790974/how-to-calculate-tfidf-for-a-single-new-document-to-be-classified">https://stackoverflow.com/questions/22790974/how-to-calculate-tfidf-for-a-single-new-document-to-be-classified</a>) notes that the text function would work fine for one text, but the inverse-document-frequency function wouldn&rsquo;t apply as easily.</li>
<li>Since I had to switch gears with DeepSpeech, I only have one transcript to work with at this time. So then I ran it through Voyant to get some text frequency data to create a prototype, but I like the customization option of using R so that will be the tool I use for the corpus.</li>
</ul>
</li>
</ul>
<h2 id="why-use-tf-idf-for-oral-history-transcripts">Why use TF-IDF for Oral History Transcripts?<a hidden class="anchor" aria-hidden="true" href="#why-use-tf-idf-for-oral-history-transcripts">#</a></h2>
<ul>
<li>I know there are limitations in both the tool and using transcripts to identify textual patterns. Computational analysis tools can be a great way to help users get a feel for the content of an OH collection. It can present a powerful means of understanding document similarity by identifying key terms across multiple transcripts. Unfortunately I don&rsquo;t have a prototype resulting from the IDF application, but I do have one showing TF patterns in one interview.</li>
</ul>
<h2 id="plan-2-prototype-process">Plan 2: Prototype Process<a hidden class="anchor" aria-hidden="true" href="#plan-2-prototype-process">#</a></h2>
<ul>
<li>After running the transcript through Voyant, I edited the stopwords to remove verbal pauses, and other non-descript terms. This generated a list of the 25 most frequent terms in the text, as well as a wordcloud representation of the ascending data.</li>
<li>If you&rsquo;d like to see my list of stopwords and the frequency list, <a href="https://docs.google.com/spreadsheets/d/1ABT2dhzQNmu6CMsxs7JfuKs9PPKirIaME9gAUebSfG4/edit?usp=sharing">click here</a></li>
</ul>
<h3 id="here-is-the-visualized-data-the-wordcloud">Here is the visualized data (the wordcloud)<a hidden class="anchor" aria-hidden="true" href="#here-is-the-visualized-data-the-wordcloud">#</a></h3>
<p><img loading="lazy" src="/images/Wordcloud.png" alt=""  />
</p>
<h3 id="now-the-audio-prototype">Now, the audio prototype<a hidden class="anchor" aria-hidden="true" href="#now-the-audio-prototype">#</a></h3>
<ul>
<li>
<p>I decided I wanted to create an audiofile that reflected the frequency data, which I made using only the 5 most frequent terms.</p>
</li>
<li>
<p>I then pulled samples from my interview where the participants were saying these words to add another dimension to the data.</p>
</li>
<li>
<p>I also thought it would be quite powerful to loop these words, and increase the volume for each word depending on their TF. For example, the more often the word appeared in the text, the louder the word would be. The audio was then ordered from the quietest sample to the loudest.</p>
</li>
<li>
<p>This is a very basic prototype, but here is an example of the sonal interpretation:</p>
</li>
</ul>
<figure >
  <audio controls preload="metadata">
    <source src="/posts/Transcript-Soundcloud.mp3" type="audio/mpeg">
  </audio>
  
</figure>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2021 <a href="https://dmahon30.github.io/">My Digital Mewsings Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
