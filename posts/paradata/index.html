<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Digital History Project Paradata | My Digital Mewsings Blog</title>
<meta name="keywords" content="" />
<meta name="description" content="Sounding the Transcript: Creating Soundscapes using TF-IDF Scores of Oral History Transcripts Macrodata In oral history practice, transcripts are used to quickly scan the interview for subject matter and key words, helping the listener identify thematic patterns in the content (Gref, Kohler, and Leh: 2018). The transcript also filters out the emotional context, tone, silences, in which meaning is expressed (High, 194).
With the digital turn, databases have made oral histories more accessible and searchable, by tethering transcripts to the recording to help bridge the gap between transcription and context  (Luo, Xiao, &amp; High: 2013).">
<meta name="author" content="">
<link rel="canonical" href="https://dmahon30.github.io/posts/paradata/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://dmahon30.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dmahon30.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dmahon30.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dmahon30.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://dmahon30.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Digital History Project Paradata" />
<meta property="og:description" content="Sounding the Transcript: Creating Soundscapes using TF-IDF Scores of Oral History Transcripts Macrodata In oral history practice, transcripts are used to quickly scan the interview for subject matter and key words, helping the listener identify thematic patterns in the content (Gref, Kohler, and Leh: 2018). The transcript also filters out the emotional context, tone, silences, in which meaning is expressed (High, 194).
With the digital turn, databases have made oral histories more accessible and searchable, by tethering transcripts to the recording to help bridge the gap between transcription and context  (Luo, Xiao, &amp; High: 2013)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dmahon30.github.io/posts/paradata/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-08T08:42:40-04:00" />
<meta property="article:modified_time" content="2021-12-08T08:42:40-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Digital History Project Paradata"/>
<meta name="twitter:description" content="Sounding the Transcript: Creating Soundscapes using TF-IDF Scores of Oral History Transcripts Macrodata In oral history practice, transcripts are used to quickly scan the interview for subject matter and key words, helping the listener identify thematic patterns in the content (Gref, Kohler, and Leh: 2018). The transcript also filters out the emotional context, tone, silences, in which meaning is expressed (High, 194).
With the digital turn, databases have made oral histories more accessible and searchable, by tethering transcripts to the recording to help bridge the gap between transcription and context  (Luo, Xiao, &amp; High: 2013)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dmahon30.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Digital History Project Paradata",
      "item": "https://dmahon30.github.io/posts/paradata/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Digital History Project Paradata",
  "name": "Digital History Project Paradata",
  "description": "Sounding the Transcript: Creating Soundscapes using TF-IDF Scores of Oral History Transcripts Macrodata In oral history practice, transcripts are used to quickly scan the interview for subject matter and key words, helping the listener identify thematic patterns in the content (Gref, Kohler, and Leh: 2018). The transcript also filters out the emotional context, tone, silences, in which meaning is expressed (High, 194).\nWith the digital turn, databases have made oral histories more accessible and searchable, by tethering transcripts to the recording to help bridge the gap between transcription and context  (Luo, Xiao, \u0026amp; High: 2013).",
  "keywords": [
    
  ],
  "articleBody": "Sounding the Transcript: Creating Soundscapes using TF-IDF Scores of Oral History Transcripts Macrodata In oral history practice, transcripts are used to quickly scan the interview for subject matter and key words, helping the listener identify thematic patterns in the content (Gref, Kohler, and Leh: 2018). The transcript also filters out the emotional context, tone, silences, in which meaning is expressed (High, 194).\nWith the digital turn, databases have made oral histories more accessible and searchable, by tethering transcripts to the recording to help bridge the gap between transcription and context  (Luo, Xiao, \u0026 High: 2013).\nAs a public historian, I use oral history databases to investigate meaning in life story and oral history interviewing. Yet the problem of knowing the context behind the words transcribed still stands with databases.\nIs there a way to merge analysis and research with affect and immersion when analyzing transcripts? Could sonifying keywords add depth to an oral history database? To help answer these questions, this project urges researchers to go beyond the confines of the text to introduce soundscapes, providing the listener with another valuable layer of interpretation (Cheng, 128). I set out to embody interviewee memories through soundscape interpretations of term frequency - inverse document frequency (TF-IDF) data. Feelings of frustration, humor, or joy expressed by the interviewee can be construed through sound, giving tf-idf terms much needed meaning and context.\nThe TF-IDF soundscapes do not fall within the realm of sonification since the text data is not being converted into sound signals (Graham, 2016). Meaning is instead generated through a sonic performance derived from audio samples. Meaning is thus generated through the sonic performance of life stories attributed to key terms. As the listener tries to understand the connection between the TF-IDF term and ambient sounds, an emotional experience is generated through immersion and identification with the interviewee (Collins, 2013: 18).\nIn addition to conveying emotions often absent in text, this project allowed me as both the interviewer and transcriber to reflect upon these terms and immerse myself in my own interpretation of these life stories.\nRepresenting key words through creative sonic interpretations of transcript data could offer historians and qualitative researchers the opportunity to listen and reflect upon their interviews in new ways.\nThis project contributes to work done by other oral historians and data scientists who use text analysis to convey heterogeneous dimensions of collective memory, and the potential for digital tools to represent shifts between the collective and the individual life story. While projects like Gabor Toth’s text analysis of Holocaust testimonies explore these dimensions in great detail, I extend on such works by merging text with soundscapes (Toth, 2018-19).\nWhy the TF-IDF approach? I wanted to see how a computational method could identify key words in my interviews, to trial an efficient method for selecting key themes. What I liked about TF-IDF methods was that instead of simply selecting the most frequent terms, terms with high TF-IDF scores are unique in their frequency when compared to other documents in the corpus (Lavin, 2019).\nCould this method tell me what distinct words were specific to certain sections of the interview in comparison to the rest of the interview? TF-IDF offers a replicable method for analyzing interview transcripts, and its algorithm is both easy to understand and adapt to the researcher’s needs (for example, adding your own stopwords to manipulate the data).\nMicrodata: These are the steps for each stage of the sound design project:\nCreating TF-IDF scores using R:   I transcribed two interviews with two former Africville residents using NVivo software\n  Each transcription was then broken down into text files according to duration. This was done to create a corpus of texts for one interview.\n  I then used Dr. Graham’s tutorial on TF-IDF with R, generating an excel file with the scores and a visualization of these scores.\n  Trialing TF-IDF Sonification using MusicAlgorithms I wanted to try using MusicAlgorithms, a web-based tool for transforming data into musical notes on an 88 note scale. This experiment was to see if I could convey emotion and narrative solely through TF-IDF scores and instrumentals. I wanted to compare this process with the soundscape project to better understand why I chose to labor over the vocal and soundscape project.\n  I chose the top three highest scores for each text file in a given interview.\n  I added the scores to the pitch input as integers (they were generated as fractional numbers) by multiplying each score by 100 and rounding.\n  I set the duration input to “1, 2, 3” in order of the descending score data, and set the bpm to 70 to represent the minutes of the interview.\n  As an experiment, I was able to create mood with the data that connoted the timbre of interviewee voices and part of their personality (the piano for Paula’s interview was more staccato because she speaks quickly and with verbosity, while Lyle’s had low bass instrumentals to convey his deep voice and drawn out speech). However the MusicAlgorithms experiment did not merge text, affectivity, and life story context, and lacked that immersive quality that I found with the soundscape sampling project.\nCreating the Soundscapes:   I took the top three words from each text file in an interview and created sound samples by clipping the word from the interview and adding it to an Audacity track. The choice to include term vocalizations was inspired by Daniel Ruten’s sound cloud project.\n  I then crafted a soundscape narrative of the memory associated with each term. I pulled sounds from [freesound.org] to create a sonic storytelling experience. Click here to view the Freesound Attributions for this project. The soundscapes bleed into one another by overlapping the samples to imply the dialogical relationship of an interview setting.\n  I made sure that the order of the terms matched that of the descending TF-IDF scores (highest score first, lowest last).\n  I made sure to match the soundscapes of terms and synonyms of those terms across both interviews. This allows the listener to hear patterns in the sound, indicating that these individual memories are part of a larger, collective experience.\n  Challenges and Limitations: TF-IDF: Using text analysis to extract keywords from oral history interviews runs the risk of eliminating meaningful words or chopping up phrases. In the former case, stopwords like “they” would get excluded, however “they” was often used as a euphemism for the city. These distinctions in the data are important for identifying narrative style in text corpora (Uitenbogerd: 2019). I kept this word as a stopword because of the nature of this sonification project and the difficulties in filtering a regular pronoun from a euphemism using text analysis. However, I was able to work around the latter issue in my sonification. Words that had been split up in the scores, like “Bedford” and “Basin” were reunited in the word sample.\nSampling: To create an immersive sound experience, I selected sounds that implied movement, activity, and story. Yet at some points I focused far too much on the literal meaning of the term than on the memories attached to it. There were soundscapes that I revised to really focus on what the memory may have sounded like. For example, in Lyle’s interview, the word “prison” initially accompanied the sound of a prison cell closing. Yet the cell had nothing to do with the story, which was about kids taking vegetables from the prison garden. I had to remember that the point of this project was not to reproduce these terms in sound, but rather to immerse the listener in memory to evoke an emotional connection - to stoke curiosity and interest, hence encouraging a deeper level of listening.\nReferences:\nBroad, Nick. “Lean On Me.“YouTube, August 9, 2010. Video. https://www.youtube.com/watch?v=69-DBXiOYuo.\nCheng, Nien Yuan. “‘Flesh and Blood Archives’: Embodying the Oral History Transcript.” Oral History Review 45, no. 1 (2018): 127–42.\nCollins, Karen. Playing with Sound: A Theory of Interacting with Sound and Music in Video Games. The MIT Press, 2013.\nGraham, Shawn. “The Sound of Data (a Gentle Introduction to Sonification for Historians).” The Programming Historian, June 7, 2016. https://doi.org/10.46430/phen0057.\nGrant, Lyle. Interviewed by Danielle Mahon, October 21, 2021. Recording in possession of the interviewer.\nGrant-Smith, Paula. Interviewed by Danielle Mahon, September 1st, 2021. Recording in the possession of the interviewer.\nGref, Michael, Joachim Köhler, and Almut Leh, ed. 2018 “Improved Transcription and Indexing of Oral History Interviews for Digital Humanities Research.” Conference paper presented at the 11th International Conference on Language Resource and Evaluation, 3124-31. Miyazaki, Japan.\nHigh, Steven. Oral History at the Crossroads: Sharing Life Stories of Survival and Displacement. Vancouver: UBC Press, 2010.\nRuten, Daniel. “Sonic Word Cloud Project Part 3: Bringing It All Together in Ableton.” Learning the Ropes of Digital History (blog), April 15, 2017. https://danielruten.wordpress.com/2017/04/15/sonic-word-cloud-project-part-3-bringing-it-all-together-in-ableton/.\nToth, Gabor. 2019. “In Search of the Drowned in the Words of the Saved: Testimonial Fragments of the Holocaust.” Public lecture at University of Oxford: USC Shoah Foundation. Accessed December 8, 2021. https://sfi.usc.edu/news/2019/05/24871-%E2%80%9C-search-drowned-words-saved-testimonial-fragments-holocaust%E2%80%9D-gabor-toth-phd.\nUitdenbogerd, Alexandra L. “Word Cloud: A Prototype Data Choralification of Text Documents.” Journal of New Music Research 48, no. 3 (May 27, 2019): 253–63. https://doi.org/10.1080/09298215.2019.1606255.\nXiao, Lu, Yan Luo, and Steven High. “CKM: A Shared Visual Analytical Tool for Large-Scale Analysis of Audio-Video Interviews.” In 2013 IEEE International Conference on Big Data, 104–12. Silicon Valley, CA, USA: IEEE, 2013. https://doi.org/10.1109/BigData.2013.6691677.\n",
  "wordCount" : "1550",
  "inLanguage": "en",
  "datePublished": "2021-12-08T08:42:40-04:00",
  "dateModified": "2021-12-08T08:42:40-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dmahon30.github.io/posts/paradata/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Digital Mewsings Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dmahon30.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dmahon30.github.io/" accesskey="h" title="My Digital Mewsings Blog (Alt + H)">My Digital Mewsings Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Digital History Project Paradata
    </h1>
    <div class="post-meta"><span title='2021-12-08 08:42:40 -0400 -0400'>December 8, 2021</span>

</div>
  </header> 
  <div class="post-content"><h1 id="sounding-the-transcript-creating-soundscapes-using-tf-idf-scores-of-oral-history-transcripts">Sounding the Transcript: Creating Soundscapes using TF-IDF Scores of Oral History Transcripts<a hidden class="anchor" aria-hidden="true" href="#sounding-the-transcript-creating-soundscapes-using-tf-idf-scores-of-oral-history-transcripts">#</a></h1>
<h2 id="macrodata">Macrodata<a hidden class="anchor" aria-hidden="true" href="#macrodata">#</a></h2>
<p>In oral history practice, transcripts are used to quickly scan the interview for subject matter and key words, helping the listener identify thematic patterns in the content (Gref, Kohler, and Leh: 2018). The transcript also filters out the emotional context, tone, silences, in which meaning is expressed (High, 194).</p>
<p>With the digital turn, databases have made oral histories more accessible and searchable, by tethering transcripts to the recording to help bridge the gap between transcription and context <a href="http://livingarchivesvivantes.org"></a> (Luo, Xiao, &amp; High: 2013).</p>
<p>As a public historian, I use oral history databases to investigate meaning in life story and oral history interviewing. Yet the problem of knowing the context behind the words transcribed still stands with databases.</p>
<p>Is there a way to merge analysis and research with affect and immersion when analyzing transcripts? Could sonifying keywords add depth to an oral history database?  To help answer these questions, this project urges researchers to go beyond the confines of the text to introduce soundscapes, providing the listener with another valuable layer of interpretation (Cheng, 128). I set out to embody interviewee memories through soundscape interpretations of term frequency - inverse document frequency (TF-IDF) data. Feelings of frustration, humor, or joy expressed by the interviewee can be construed through sound, giving tf-idf terms much needed meaning and context.</p>
<p>The TF-IDF soundscapes do not fall within the realm of sonification since the text data is not being converted into sound signals (Graham, 2016). Meaning is instead generated through a sonic performance derived from audio samples. Meaning is thus generated through the sonic performance of life stories attributed to key terms. As the listener tries to understand the connection between the TF-IDF term and ambient sounds, an emotional experience is generated through immersion and identification with the interviewee (Collins, 2013: 18).</p>
<p>In addition to conveying emotions often absent in text, this project allowed me as both the interviewer and transcriber to reflect upon these terms and immerse myself in my own interpretation of these life stories.</p>
<p>Representing  key words through creative sonic interpretations of transcript data could offer  historians and qualitative researchers the opportunity to listen and reflect upon their interviews in new ways.</p>
<p>This project contributes to work done by other oral historians and data scientists who use text analysis to convey heterogeneous dimensions of collective memory, and the potential for digital tools to represent shifts between the collective and the individual life story. While projects like Gabor Toth’s text analysis of Holocaust testimonies explore these dimensions in great detail, I extend on such works by merging text with soundscapes (Toth, 2018-19).</p>
<h3 id="why-the-tf-idf-approach">Why the TF-IDF approach?<a hidden class="anchor" aria-hidden="true" href="#why-the-tf-idf-approach">#</a></h3>
<p>I wanted to see how a computational method could identify key words in my interviews, to trial an efficient method for selecting key themes. What I liked about TF-IDF methods was that instead of simply selecting the most frequent terms, terms with high TF-IDF scores are unique in their frequency when compared to other documents in the corpus (Lavin, 2019).</p>
<p>Could this method tell me what distinct words were specific to certain sections of the interview in comparison to the rest of the interview? TF-IDF offers a replicable method for analyzing interview transcripts, and its algorithm is both easy to understand and adapt to the researcher’s needs (for example, adding your own stopwords to manipulate the data).</p>
<h2 id="microdata">Microdata:<a hidden class="anchor" aria-hidden="true" href="#microdata">#</a></h2>
<p>These are the steps for each stage of the sound design project:</p>
<h3 id="creating-tf-idf-scores-using-r">Creating TF-IDF scores using R:<a hidden class="anchor" aria-hidden="true" href="#creating-tf-idf-scores-using-r">#</a></h3>
<ol>
<li>
<p>I transcribed two interviews with two former Africville residents using NVivo software</p>
</li>
<li>
<p>Each transcription was then broken down into text files according to duration. This was done to create a corpus of texts for one interview.</p>
</li>
<li>
<p>I then used Dr. Graham’s tutorial on TF-IDF with R, generating an excel file with the scores and a visualization of these scores.</p>
</li>
</ol>
<h3 id="trialing-tf-idf-sonification-using-musicalgorithms">Trialing TF-IDF Sonification using MusicAlgorithms<a hidden class="anchor" aria-hidden="true" href="#trialing-tf-idf-sonification-using-musicalgorithms">#</a></h3>
<p>I wanted to try using MusicAlgorithms, a web-based tool for transforming data into musical notes on an 88 note scale. This experiment was to see if I could convey emotion and narrative solely through TF-IDF scores and instrumentals. I wanted to compare this process with the soundscape project to better understand why I chose to labor over the vocal and soundscape project.</p>
<ol>
<li>
<p>I chose the top three highest scores for each text file in a given interview.</p>
</li>
<li>
<p>I added the scores to the pitch input as integers (they were generated as fractional numbers) by multiplying each score by 100 and rounding.</p>
</li>
<li>
<p>I set the duration input to “1, 2, 3” in order of the descending score data, and set the bpm to 70 to represent the minutes of the interview.</p>
</li>
</ol>
<p>As an experiment, I was able to create mood with the data that connoted the timbre of interviewee voices and part of their personality (the piano for Paula’s interview was more staccato because she speaks quickly and with verbosity, while Lyle’s had low bass instrumentals to convey his deep voice and drawn out speech). However the MusicAlgorithms experiment did not merge text, affectivity, and life story context, and lacked that immersive quality that I found with the soundscape sampling project.</p>
<h3 id="creating-the-soundscapes">Creating the Soundscapes:<a hidden class="anchor" aria-hidden="true" href="#creating-the-soundscapes">#</a></h3>
<ol>
<li>
<p>I took the top three words from each text file in an interview and created sound samples by clipping the word from the interview and adding it to an Audacity track. The choice to include term vocalizations was inspired by <a href="https://danielruten.wordpress.com/2017/04/15/sonic-word-cloud-project-part-3-bringing-it-all-together-in-ableton/">Daniel Ruten’s sound cloud project</a>.</p>
</li>
<li>
<p>I then crafted a soundscape narrative of the memory associated with each term. I pulled sounds from [freesound.org] to create a sonic storytelling experience. Click <a href="https://docs.google.com/document/d/1jSBHlBnp6c84NZb6N8VqBfJsYfkwVPRcWzwIapH3yOw/edit?usp=sharing">here</a> to view the Freesound Attributions for this project.
The soundscapes bleed into one another by overlapping the samples to imply the dialogical relationship of an interview setting.</p>
</li>
<li>
<p>I made sure that the order of the terms matched that of the descending TF-IDF scores (highest score first, lowest last).</p>
</li>
<li>
<p>I made sure to match the soundscapes of terms and synonyms of those terms across both interviews. This allows the listener to hear patterns in the sound, indicating that these individual memories are part of a larger, collective experience.</p>
</li>
</ol>
<h3 id="challenges-and-limitations">Challenges and Limitations:<a hidden class="anchor" aria-hidden="true" href="#challenges-and-limitations">#</a></h3>
<p><em>TF-IDF:</em> Using text analysis to extract keywords from oral history interviews runs the risk of eliminating meaningful words or chopping up phrases. In the former case, stopwords like “they” would get excluded, however “they” was often used as a euphemism for the city. These distinctions in the data are important for identifying narrative style in text corpora (Uitenbogerd: 2019). I kept this word as a stopword because of the nature of this sonification project and the difficulties in filtering a regular pronoun from a euphemism using text analysis. However, I was able to work around the latter issue in my sonification. Words that had been split up in the scores, like “Bedford” and “Basin” were reunited in the word sample.</p>
<p><em>Sampling</em>: To create an immersive sound experience, I selected sounds that implied movement, activity, and story. Yet at some points I focused far too much on the literal meaning of the term than on the memories attached to it. There were soundscapes that I revised to really focus on what the memory may have sounded like. For example, in Lyle’s interview, the word “prison” initially accompanied the sound of a prison cell closing. Yet the cell had nothing to do with the story, which was about kids taking vegetables from the prison garden. I had to remember that the point of this project was not to reproduce these terms in sound, but rather to immerse the listener in memory to evoke an emotional connection - to stoke curiosity and interest, hence encouraging a deeper level of listening.</p>
<p>References:</p>
<p>Broad, Nick. &ldquo;Lean On Me.&ldquo;YouTube, August 9, 2010. Video. <a href="https://www.youtube.com/watch?v=69-DBXiOYuo">https://www.youtube.com/watch?v=69-DBXiOYuo</a>.</p>
<p>Cheng, Nien Yuan. “‘Flesh and Blood Archives’: Embodying the Oral History Transcript.” <em>Oral History Review 45</em>, no. 1 (2018): 127–42.</p>
<p>Collins, Karen. <em>Playing with Sound: A Theory of Interacting with Sound and Music in Video Games.</em> The MIT Press, 2013.</p>
<p>Graham, Shawn. “The Sound of Data (a Gentle Introduction to Sonification for Historians).” The Programming Historian, June 7, 2016. <a href="https://doi.org/10.46430/phen0057">https://doi.org/10.46430/phen0057</a>.</p>
<p>Grant, Lyle. Interviewed by Danielle Mahon, October 21, 2021. Recording in possession of the interviewer.</p>
<p>Grant-Smith, Paula. Interviewed by Danielle Mahon, September 1st, 2021. Recording in the possession of the interviewer.</p>
<p>Gref, Michael, Joachim Köhler, and Almut Leh, ed. 2018 &ldquo;Improved Transcription and Indexing of Oral History Interviews for Digital Humanities Research.” Conference paper presented at the <em>11th International Conference on Language Resource and Evaluation</em>, 3124-31. Miyazaki, Japan.</p>
<p>High, Steven. <em>Oral History at the Crossroads: Sharing Life Stories of Survival and Displacement</em>. Vancouver: UBC Press, 2010.</p>
<p>Ruten, Daniel. “Sonic Word Cloud Project Part 3: Bringing It All Together in Ableton.” Learning the Ropes of Digital History (blog), April 15, 2017.
<a href="https://danielruten.wordpress.com/2017/04/15/sonic-word-cloud-project-part-3-bringing-it-all-together-in-ableton/">https://danielruten.wordpress.com/2017/04/15/sonic-word-cloud-project-part-3-bringing-it-all-together-in-ableton/</a>.</p>
<p>Toth, Gabor. 2019. &ldquo;In Search of the Drowned in the Words of the Saved: Testimonial Fragments of the Holocaust.&rdquo; Public lecture at <em>University of Oxford: USC Shoah Foundation.</em> Accessed December 8, 2021. <a href="https://sfi.usc.edu/news/2019/05/24871-%E2%80%9C-search-drowned-words-saved-testimonial-fragments-holocaust%E2%80%9D-gabor-toth-phd">https://sfi.usc.edu/news/2019/05/24871-%E2%80%9C-search-drowned-words-saved-testimonial-fragments-holocaust%E2%80%9D-gabor-toth-phd</a>.</p>
<p>Uitdenbogerd, Alexandra L. “Word Cloud: A Prototype Data Choralification of Text Documents.” <em>Journal of New Music Research</em> 48, no. 3 (May 27, 2019): 253–63. <a href="https://doi.org/10.1080/09298215.2019.1606255">https://doi.org/10.1080/09298215.2019.1606255</a>.</p>
<p>Xiao, Lu, Yan Luo, and Steven High. “CKM: A Shared Visual Analytical Tool for Large-Scale Analysis of Audio-Video Interviews.” In <em>2013 IEEE International Conference on Big Data</em>, 104–12. Silicon Valley, CA, USA: IEEE, 2013. <a href="https://doi.org/10.1109/BigData.2013.6691677">https://doi.org/10.1109/BigData.2013.6691677</a>.</p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2021 <a href="https://dmahon30.github.io/">My Digital Mewsings Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
